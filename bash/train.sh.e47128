wandb: Currently logged in as: orangebai. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /storage/hpc/52/jiangz9/Experiments/AP_Prune/cifar10/vgg16/prune_l1/wandb/run-20230514_150247-3emmbn8r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run l1_5_0.95
wandb: ⭐️ View project at https://wandb.ai/orangebai/prune_l1
wandb: 🚀 View run at https://wandb.ai/orangebai/prune_l1/runs/3emmbn8r
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /mmfs1/storage/users/jiangz9/Experiments/AP_Prune/cifar10/vgg16/prune_l1/wandb/run-20230514_150247-3emmbn8r/files exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name          | Type             | Params
---------------------------------------------------
0 | model         | VGG16            | 33.7 M
1 | loss_function | CrossEntropyLoss | 0     
---------------------------------------------------
33.7 M    Trainable params
0         Non-trainable params
33.7 M    Total params
134.635   Total estimated model params size (MB)
wandb: 429 encountered (Filestream rate limit exceeded, retrying in 2.0999630070377 seconds), retrying request
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆███████
wandb:                  lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     sparsity/global ▁█
wandb:    sparsity/layer_0 ▁█
wandb:    sparsity/layer_1 ▁█
wandb:   sparsity/layer_10 ▁█
wandb:   sparsity/layer_11 ▁█
wandb:   sparsity/layer_12 ▁█
wandb:   sparsity/layer_13 ▁█
wandb:   sparsity/layer_14 ▁█
wandb:    sparsity/layer_2 ▁█
wandb:    sparsity/layer_3 ▁█
wandb:    sparsity/layer_4 ▁█
wandb:    sparsity/layer_5 ▁█
wandb:    sparsity/layer_6 ▁█
wandb:    sparsity/layer_7 ▁█
wandb:    sparsity/layer_8 ▁█
wandb:    sparsity/layer_9 ▁█
wandb:                step ▁█
wandb:          train/loss ▄▃▃▂▂▂▁█▇▆▅▆▅▅▅▅▅▅▅▅▄▅▄▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:          train/top1 ▅▆▆▇███▂▂▃▃▃▄▅▄▅▃▄▅▄▅▄▅▂▁▁▂▁▁▁▁▁▁▂▁▂▂▂▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███
wandb:            val/loss ▁▁▁█▄
wandb:            val/top1 █▄▅▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 4
wandb:                  lr 0.0012
wandb:     sparsity/global 0.9975
wandb:    sparsity/layer_0 0.99665
wandb:    sparsity/layer_1 0.99745
wandb:   sparsity/layer_10 0.9975
wandb:   sparsity/layer_11 0.9975
wandb:   sparsity/layer_12 0.9975
wandb:   sparsity/layer_13 0.9975
wandb:   sparsity/layer_14 0.9975
wandb:    sparsity/layer_2 0.99748
wandb:    sparsity/layer_3 0.99749
wandb:    sparsity/layer_4 0.9975
wandb:    sparsity/layer_5 0.9975
wandb:    sparsity/layer_6 0.9975
wandb:    sparsity/layer_7 0.9975
wandb:    sparsity/layer_8 0.9975
wandb:    sparsity/layer_9 0.9975
wandb:                step 1173
wandb:          train/loss 2.30192
wandb:          train/top1 8.59375
wandb: trainer/global_step 1954
wandb:            val/loss 652.37897
wandb:            val/top1 10.0
wandb: 
wandb: Synced l1_5_0.95: https://wandb.ai/orangebai/prune_l1/runs/3emmbn8r
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: /storage/hpc/52/jiangz9/Experiments/AP_Prune/cifar10/vgg16/prune_l1/wandb/run-20230514_150247-3emmbn8r/logs
Traceback (most recent call last):
  File "train.py", line 40, in <module>
    trainer.fit(model)
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 603, in fit
    call._call_and_handle_interrupt(
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 645, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1098, in _run
    results = self._run_stage()
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1177, in _run_stage
    self._run_train()
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1200, in _run_train
    self.fit_loop.run()
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.on_advance_end()
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 296, in on_advance_end
    self.trainer._call_lightning_module_hook("on_train_epoch_end")
  File "/mmfs1/storage/users/jiangz9/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1342, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/mmfs1/home/users/jiangz9/Code/AP_Prune/core/pl_model.py", line 117, in on_train_epoch_end
    block.compute_mask(adjusted_amount[name] * self.args.amount)
  File "/mmfs1/home/users/jiangz9/Code/AP_Prune/models/blocks.py", line 57, in compute_mask
    selected = np.random.choice(len(all_equal_indices[0]), equal_to, replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
